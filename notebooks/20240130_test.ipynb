{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549333eb-caf6-4789-9977-d98d1098fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LightGBM, CrossValidationを利用したベースラインモデル.\"\"\"\n",
    "import gc\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962b40c1-932c-4d47-a0c3-beb360b4ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共通設定\n",
    "DATA_PATH = \"../data\"\n",
    "RESULT_PATH = \"../results\"\n",
    "SUMMARY_FILENAME = \"summary.csv\"\n",
    "# 個別設定\n",
    "# EXPERIMENT_NAME = os.path.splitext(os.path.basename(__file__))[0]\n",
    "MEMO = \"20240128_01.pyと一緒.segmentation faultが出る原因調査\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd351d1b-1e1a-4b87-99fe-91f969a15dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"データの前処理.\"\"\"\n",
    "    # 金額をobj->int型へ変換\n",
    "    dollar_amount_cols = [\"DisbursementGross\", \"GrAppv\", \"SBA_Appv\"]\n",
    "    for col in dollar_amount_cols:\n",
    "        df[col] = df[col].apply(lambda x: x.replace(\"$\", \"\").replace(\".\", \"\").replace(\",\", \"\")).astype(int).copy()\n",
    "    # 年月関連を年と月で分ける\n",
    "    ymd_cols = [\"DisbursementDate\", \"ApprovalDate\"]\n",
    "    for col in ymd_cols:\n",
    "        df[col + \"_year\"] = pd.to_datetime(df[col]).apply(lambda x: x.year)\n",
    "        df[col + \"_month\"] = pd.to_datetime(df[col]).apply(lambda x: x.month)\n",
    "        df = df.drop(columns=col)\n",
    "    # category型への変換\n",
    "    obj_cols = df.select_dtypes(include=object).columns\n",
    "    df[obj_cols] = df[obj_cols].astype(\"category\").copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def postprocess_prediction(y_pred_proba, negative_ratio):\n",
    "    threshold = np.sort(y_pred_proba)[int(y_pred_proba.shape[0] * negative_ratio)]\n",
    "    y_pred = np.where(y_pred_proba < threshold, 0, 1)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def save_cv_result(result_df: pd.DataFrame):\n",
    "    summary_path = os.path.join(RESULT_PATH, SUMMARY_FILENAME)\n",
    "    if os.path.exists(summary_path):\n",
    "        df = pd.read_csv(summary_path)\n",
    "    else:\n",
    "        df = pd.DataFrame({\"experiment_name\": [], \"cv_score\": [], \"MEMO\": [], \"board_score\": []})\n",
    "    df = pd.concat([df, result_df]).drop_duplicates()\n",
    "    df.to_csv(summary_path, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # データ読み込み\n",
    "    train_data = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"), index_col=0)\n",
    "    test_data = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"), index_col=0)\n",
    "\n",
    "    # frequency encoding\n",
    "    encoding_target_cols = [\"FranchiseCode\", \"RevLineCr\", \"LowDoc\", \"UrbanRural\", \"State\", \"BankState\", \"City\", \"Sector\"]\n",
    "    for col in encoding_target_cols:\n",
    "        count_dict = dict(train_data[col].value_counts())\n",
    "        train_data[f\"{col}_freq_encoding\"] = train_data[col].map(count_dict)\n",
    "        test_data[f\"{col}_freq_encoding\"] = test_data[col].map(count_dict).fillna(1).astype(int)\n",
    "    # 前処理\n",
    "    train_data = preprocess_data(train_data)\n",
    "    test_data = preprocess_data(test_data)\n",
    "\n",
    "    # 説明変数と目的変数に分ける\n",
    "    X = train_data.drop(columns=\"MIS_Status\").copy()\n",
    "    y = train_data[\"MIS_Status\"].copy()\n",
    "    X_test = test_data.copy()\n",
    "\n",
    "    # ハイパーパラメータの設定\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "    }\n",
    "\n",
    "    # cross validation\n",
    "    kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    scores = []\n",
    "    for i, (train_eval_post_index, valid_index) in enumerate(kf.split(train_data)):\n",
    "        # 学習データ，early_stopping用，評価用に分割\n",
    "        X_train_eval_post = X.iloc[train_eval_post_index].copy()\n",
    "        y_train_eval_post = y.iloc[train_eval_post_index].copy()\n",
    "        X_train_eval, X_post, y_train_eval, y_post = train_test_split(X_train_eval_post, y_train_eval_post, test_size=0.2, random_state=42)\n",
    "        X_train, X_eval, y_train, y_eval = train_test_split(X_train_eval, y_train_eval, test_size=0.25, random_state=42)\n",
    "        X_valid = X.iloc[valid_index].copy()\n",
    "        y_valid = y.iloc[valid_index].copy()\n",
    "\n",
    "        # lightgbm用データセットに変換\n",
    "        train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "        eval_dataset = lgb.Dataset(X_eval, label=y_eval)\n",
    "\n",
    "        # LightGBMモデルの学習\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_dataset,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[train_dataset, eval_dataset],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True)],\n",
    "        )\n",
    "\n",
    "        # 後処理の閾値決定\n",
    "        y_pred_proba = model.predict(X_post)\n",
    "        best_score = 0\n",
    "        negative_ratio = 0\n",
    "        for i in np.linspace(0, 0.3, 30):\n",
    "            y_pred = postprocess_prediction(y_pred_proba, i)\n",
    "            score = f1_score(y_post, y_pred, average=\"macro\")\n",
    "            if score > best_score:\n",
    "                negative_ratio = i\n",
    "                best_score = score\n",
    "\n",
    "        # 評価用データの予測\n",
    "        y_pred_proba = model.predict(X_valid)\n",
    "        y_pred = postprocess_prediction(y_pred_proba, negative_ratio)\n",
    "        score = f1_score(y_valid, y_pred, average=\"macro\")\n",
    "        scores.append(score)\n",
    "\n",
    "    # CVの結果の保存\n",
    "    # result_df = pd.DataFrame({\"experiment_name\": [EXPERIMENT_NAME], \"cv_score\": [np.mean(scores)], \"MEMO\": [MEMO], \"board_score\": [None]})\n",
    "    # save_cv_result(result_df)\n",
    "\n",
    "    # テストデータへの予測\n",
    "    X_train_eval, X_valid, y_train_eval, y_valid = train_test_split(X, y, test_size=0.21, random_state=42)\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(X_train_eval, y_train_eval, test_size=0.21, random_state=42)\n",
    "    train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "    eval_dataset = lgb.Dataset(X_eval, label=y_eval)\n",
    "    # 学習\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_dataset,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_dataset, eval_dataset],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True)],\n",
    "    )\n",
    "    # 後処理の閾値決定\n",
    "    y_pred_proba = model.predict(X_valid)\n",
    "    best_score = 0\n",
    "    negative_ratio = 0\n",
    "    for i in np.linspace(0, 0.3, 30):\n",
    "        y_pred = postprocess_prediction(y_pred_proba, i)\n",
    "        score = f1_score(y_valid, y_pred, average=\"macro\")\n",
    "        if score > best_score:\n",
    "            negative_ratio = i\n",
    "            best_score = score\n",
    "\n",
    "    # 予測\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    return y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebff05b-a7a8-4239-a60b-0ded31c7cbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18104, number of negative: 2203\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2860\n",
      "[LightGBM] [Info] Number of data points in the train set: 20307, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.891515 -> initscore=2.106313\n",
      "[LightGBM] [Info] Start training from score 2.106313\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\ttraining's binary_logloss: 0.253888\tvalid_1's binary_logloss: 0.280417\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18135, number of negative: 2172\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2877\n",
      "[LightGBM] [Info] Number of data points in the train set: 20307, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.893042 -> initscore=2.122195\n",
      "[LightGBM] [Info] Start training from score 2.122195\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttraining's binary_logloss: 0.2394\tvalid_1's binary_logloss: 0.278328\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18122, number of negative: 2185\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2894\n",
      "[LightGBM] [Info] Number of data points in the train set: 20307, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.892402 -> initscore=2.115511\n",
      "[LightGBM] [Info] Start training from score 2.115511\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's binary_logloss: 0.231046\tvalid_1's binary_logloss: 0.285059\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18147, number of negative: 2160\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2883\n",
      "[LightGBM] [Info] Number of data points in the train set: 20307, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.893633 -> initscore=2.128397\n",
      "[LightGBM] [Info] Start training from score 2.128397\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's binary_logloss: 0.246808\tvalid_1's binary_logloss: 0.283819\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Number of positive: 18198, number of negative: 2109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2867\n",
      "[LightGBM] [Info] Number of data points in the train set: 20307, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.896144 -> initscore=2.155098\n",
      "[LightGBM] [Info] Start training from score 2.155098\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttraining's binary_logloss: 0.243583\tvalid_1's binary_logloss: 0.293147\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3541e7-9a4e-4791-ab07-a376e244b3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
